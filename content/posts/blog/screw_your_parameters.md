Title: Screw Your Parameters
Date: 2015-8-04 08:15
Modified: 2015-8-08 21:57
Category: Blog
Tags: galvanize, data science
Slug: non-parametric-learners
Authors: Cary Goltermann
Summary: If the world isn't as simple as place as regressions assume it to be, how can we do a decent job modeling it? Get rid of all the parameters.

Last post I talked about [linear and logistic regression models](|filename|./regressing_to_regressions.md), they were fairly simple and easy to understand, qualities whose value cannot be understated. But pretend there are some interactions between the variables that describe the world that can't be described by scaling and exponentiating. This is obviously the case, e.g. whether or not you go to the park to play volleyball with your friends might depend on how hot it is outside, how tired you are, and how much beer is going to be there. But consider that you're friends are more likely to bring beer if it is hotter out. How can we try and predict whether or not you'll go to the park?

This is where non-parametric models do a really good job because they don't assume anything about the nature of the way a variable might affect an outcome, the way they can fit data ends up being much less constrained than the regression models. Before I get into any detail about all the different types of models like this, I want to quickly point out the two types of problems they are trying to solve, classification and regression.

- Classification tries to solve the problem, which type of known category does an observation fit into. E.g. Will a person get accepted to college or not.
- Regression tries to predict some sort of output value on a continuous scale. E.g. How many pizzas will a Dominoes make in a certain evening.

With that specification in mind let's get to talking about some models.

### K-Nearest Neighbors
K-Nearest neighbors, or kNN, is a classification algorithm that is extraordinarily simple to understand. Basically you have a data set with known classifications. No training is really necessary to classify new points, all you have to do is check which of the $k$ known data points are closest to you're unclassified point and then simply check they're classifications and classify you're new point as the class that occurs most frequently amongst the $k$ closest points.

It's kind of like playing Marco Polo, where the objective is not to catch anyone, but instead find out what the people closest to you are like. Imagine you're a point that gets thrown in with a bunch of classified points. You have no idea what class you belong to (and this matters a ton because as a human, you desperately need to belong to a group), so you scream out:

>" _Hey! What class do you guys belong to??_ " Thinking that the people closest to you are most like you and therefore will be able to tell you about the class you belong in.

 Now, depending on how loud he screamed, a certain number, $k$, of nearby peeps will have heard him. Now imagine that all at once those $\textit{k-nearest neighbors}$ scream back the name of the class they belong to, and you hearing, seemingly, a cacophony of classes, assume you belong to the one that you can hear most clearly, the one that the highest number of $\textit{k-nearest neighbors}$ screamed.

That's basically the gist of it. There's one fairly unfortunate quirk when $k$ is an even number. The simplest algorithm says, if you get equal representation of classes when they scream back, just randomly choose one. I don't particularly like this solution, so stretching the analogy, we could day that its harder to hear the neighbors that screamed from further away so they carry less weight in your consideration of class. This akin to saying that how much you weigh each neighbor's vote should be inversely proportional their distance from you.

That wasn't so painful, now was it? Nope! However, given this method's simplicity, you'd imagine that it has some shortcomings; and you'd be right! The one foremost in my mind is that the method is highly compute intensive. For every point that you're trying to classify you need to check all the points in the training set. In this sense, $kNN$ requires no training of the model up front; however you pay for that convenience every time you want to classify something.

So what if we want to spend some time up front training a model so that it can quickly classify any number of unknown observations. That's certainly what we achieved with our linear and logistic regression models, but we're living in a land without parameters here... what are we to do?

### Decision Trees
Decisions trees are basically algor(ithm)izing one of the processes that would naturally come to mind if you were to try and classify a new point based on some old data. At least I know that it seems natural to me. If I were given a problem like that, I would look at the data, check to see which of the features have values which correspond (a least decently well) with the outcome I am trying to predict. Then once I've figured out the best feature and way to split the data, I do that again, except this time I'm looking at the two groups that were created from the previous split. Then I keep doing this until I can't get any more predictive power by performing another split or I can predict perfectly. (though, I guess the latter is just a specific instance of the former...)

Mathematically, this means that each time you go to make a new branch in your decision tree you need to analyze all of possible split options to decide which is best. Best in this case can be defined by  a number of different metrics, the Gini impurity, $$G = \sum_{k=1}^K\hat{p}_{mk}(1-\hat{p}_{mk})$$ or the cross-entropy, $$D = -\sum_{k=1}^K\hat{p}_{mk}log\hat{p}_{mk}.$$ Where $\hat{p}_{mk}$ is the proportion of training observations in the *m*th branch that are from the *k*th class. Using one of these measures we can compare how much better the Gini impurity or entropy gets for each possible split as the information gain, $$Gain(S, D) = H(S) - \sum_{V\in D} \frac{|V|}{|S|}H(V),$$ where $S$ is the original set, $D$ represents all the possible splits of $S$, and $V$ are the disjoint subsets of $S$. It's also worth mentioning that this method can be adapted for regression (predicting something with continuous values as opposed to categorical ones), where you're splitting trying to minimize $RSS$, residual sum of squares, to crate $J$ regions in the continuous space that the data's features exist, from the set $S$ of possible splits, aka, in math-ese, find the $J's$ in $$\inf_{J \in S} \sum_{j=1}^J\sum_{i\in{R_j}} (y_i - \hat{y}_{R_j})^2.$$ Also, it's good to know that the leaves in regression trees typically represent the mean of the target values for the elements that are in that leaf.

Quick decision tree terminology lesson. In case you haven't noticed yet, the tree metaphor has been pretty strictly adhered to thus far, and that pattern isn't going to change very much (wait till we get to [random forests](|filename|./random_forests.md)). So more tree words applied to those of the decision type are leaves, these are terminal nodes of branches (aka the ends of branches, you know, where you'd expect to find leaves), there is a concept of depth, which is how many branches you'd have to traverse to get to a leaf, or max depth, the highest depth value on the tree, and a bunch of other, fairly, self-explanatory names that I can't think of right now.

So yeah, those are the basics of decision trees. So let's talk pros and cons. Decision trees are fairly opposite, in my mind, in terms of where the computational effort gets applied, when compared to $kNN$. Here you have to spend a ton of time analyzing all of the possible splits, for all the current branches in the tree, every time you want to add another branch. That can be pretty, to very, compute intensive; but what you get out in the end is a model that is super fast at predicting. All you have to do is follow the branches that coincide with the values that you're new data point has, and when you get to a leaf, the model predicts the most represented category of the elements in that leaf, or the mean of the target values of the elements in that leaf, for classification and regression trees, respectively. Whaaaaaa BAM! I'd like to see Mother Willow do that!

### Notes
I'm definitely back to feeling tired a lot. Luckily we have a week off after this coming one. So I plan on using that to do some recharging and learning some stuff tangentially related data science, hopefully some D3 will happen that week. We'll see, it might just be me trying to get my hands on some data for my capstone project and catch up with how behind I am with my blog posts.

Speaking of the capstone, I had a good 1-on-1 meeting with the DSI instructor, Dan, yesterday. A good portion of the talk was devoted to getting me on my way with deciding on a capstone project. One of the things that is especially valuable, in my mind, about the programs at Galvanize is that a large part of the company's business model is related to making good connections with companies, and those connects with the tech sphere as a whole. In general, I'm well aware of this fact, but yesterday during the meeting, it was made very apparent to me, its true value. Once I told Dan that I'm interest in working in the personal health sphere, Dan started rattling off names of companies, specific to that space, that he was going to put me in contact with in the hopes that I'll be able to get some data from one of these sources. In fact, we subsequently walked to a different floor and he introduced me to a guy that I'm hoping will be a great contact for this purpose. To say the least, I was pretty jazzed.

We also talked about stuff that I generally put into my weekly feedback forms, and some about how I'm doing in the class. In general, it's nice to know that the program I'm in isn't stagnant and uncaring about improving the way it teaches data science. I know that many of the things I comment on are impossible to change in the next 8 weeks that I have left in the program; but hopefully, by providing well thought out feedback, the next cohort will be able to have an even better experience than I'm having with the program.

Other random stuff. I rowed twice this past week. Managed to meet that goal that I set, well, at this point, nearly 3 weeks ago of 4750 meters in 20 minutes. The next workout I even hit 4850, so hopefully I'll be able to hit 5K in a few workouts. Then I'll start to try and mix in some interval training and possibly some other painful sounding workouts.

I'm just now seeing how long this post turned out being. I don't know if this is a good or a bad thing, but I do know that if I split up the topics a little more I would be able to publish content on a more regular basis. Seeing as I haven't been too great on following through with commitments lately, I'm going to try to make this one more achievable and go with, only cover one topic for the next post. I think that it's going to be on random forests. Until then, I'm going to Go-go-gadget dream about scotch!
